{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro:\n",
    "\n",
    "In this jupyter notebook, we are going to code some crucial parts of the diffusion process and train a diffusion model on MNIST dataset.\n",
    "\n",
    "We will mainly base our code on the [original paper](https://arxiv.org/abs/2006.11239).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "transform_to_tensor = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = MNIST(\"./MNIST_dataset/\", transform=transform_to_tensor, train= True, download=True)\n",
    "val_dataset = MNIST(\"./MNIST_dataset/\", transform=transform_to_tensor, train= False, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch_data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = torch_data.DataLoader(dataset=val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the data ? Size, shape, type, etc.\n",
    "Please plot some nice figures to compare the soon-to-be generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model (not the most important part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I shamelessly copied the code from the Internet. It is a simple Unet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNET BLOCK\n",
    "\n",
    "Not really conventional block. Usually, we should only get 1 convolution ber block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, shape, in_c, out_c, kernel_size=3, stride=1, padding=1, activation=None, normalize=True):\n",
    "        super(UNetBlock, self).__init__()\n",
    "        self.ln = nn.LayerNorm(shape)\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size, stride, padding)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size, stride, padding)\n",
    "        self.activation = nn.SiLU() if activation is None else activation\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.ln(x) if self.normalize else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding of time step\n",
    "\n",
    "Exactly as in transformer, we encode the time step into an embedding. This is to help the model to learn the temporal dependency during the training and the generation step. We use the usual sinusoidal positional embedding but we can use any other positional embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "This is the final model. We use the UNET block and the positional embedding to define the model. Please note that in diffusion, the main interest is not the model architecture but the diffusion process. The model is just a tool to help us to learn the diffusion process. Any other model should work ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyUNet(nn.Module):\n",
    "    def __init__(self, time_emb_dim=100):\n",
    "        super(MyUNet, self).__init__()\n",
    "\n",
    "        # Sinusoidal embedding\n",
    "        self.time_embed = SinusoidalPosEmb(time_emb_dim)\n",
    "        \n",
    "        # First half\n",
    "        self.te1 = mlp_time_embedding(time_emb_dim, 1)\n",
    "        self.b1 = nn.Sequential(\n",
    "            UNetBlock((1, 28, 28), 1, 10),\n",
    "            UNetBlock((10, 28, 28), 10, 10),\n",
    "            UNetBlock((10, 28, 28), 10, 10)\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(10, 10, 4, 2, 1)\n",
    "\n",
    "        self.te2 = mlp_time_embedding(time_emb_dim, 10)\n",
    "        self.b2 = nn.Sequential(\n",
    "            UNetBlock((10, 14, 14), 10, 20),\n",
    "            UNetBlock((20, 14, 14), 20, 20),\n",
    "            UNetBlock((20, 14, 14), 20, 20)\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(20, 20, 4, 2, 1)\n",
    "\n",
    "        self.te3 = mlp_time_embedding(time_emb_dim, 20)\n",
    "        self.b3 = nn.Sequential(\n",
    "            UNetBlock((20, 7, 7), 20, 40),\n",
    "            UNetBlock((40, 7, 7), 40, 40),\n",
    "            UNetBlock((40, 7, 7), 40, 40)\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(40, 40, 2, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(40, 40, 4, 2, 1)\n",
    "        )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.te_mid = mlp_time_embedding(time_emb_dim, 40)\n",
    "        self.b_mid = nn.Sequential(\n",
    "            UNetBlock((40, 3, 3), 40, 20),\n",
    "            UNetBlock((20, 3, 3), 20, 20),\n",
    "            UNetBlock((20, 3, 3), 20, 40)\n",
    "        )\n",
    "\n",
    "        # Second half\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(40, 40, 4, 2, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.ConvTranspose2d(40, 40, 2, 1)\n",
    "        )\n",
    "\n",
    "        self.te4 = mlp_time_embedding(time_emb_dim, 80)\n",
    "        self.b4 = nn.Sequential(\n",
    "            UNetBlock((80, 7, 7), 80, 40),\n",
    "            UNetBlock((40, 7, 7), 40, 20),\n",
    "            UNetBlock((20, 7, 7), 20, 20)\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(20, 20, 4, 2, 1)\n",
    "        self.te5 = mlp_time_embedding(time_emb_dim, 40)\n",
    "        self.b5 = nn.Sequential(\n",
    "            UNetBlock((40, 14, 14), 40, 20),\n",
    "            UNetBlock((20, 14, 14), 20, 10),\n",
    "            UNetBlock((10, 14, 14), 10, 10)\n",
    "        )\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(10, 10, 4, 2, 1)\n",
    "        self.te_out = mlp_time_embedding(time_emb_dim, 20)\n",
    "        self.b_out = nn.Sequential(\n",
    "            UNetBlock((20, 28, 28), 20, 10),\n",
    "            UNetBlock((10, 28, 28), 10, 10),\n",
    "            UNetBlock((10, 28, 28), 10, 10, normalize=False)\n",
    "        )\n",
    "\n",
    "        self.conv_out = nn.Conv2d(10, 1, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # x is (N, 1, 28, 28) (image with positional embedding stacked on channel dimension)\n",
    "        t = self.time_embed(t)\n",
    "        n = len(x)\n",
    "        out1 = self.b1(x + self.te1(t).reshape(n, -1, 1, 1))  # (N, 10, 28, 28)\n",
    "        out2 = self.b2(self.down1(out1) + self.te2(t).reshape(n, -1, 1, 1))  # (N, 20, 14, 14)\n",
    "        out3 = self.b3(self.down2(out2) + self.te3(t).reshape(n, -1, 1, 1))  # (N, 40, 7, 7)\n",
    "\n",
    "        out_mid = self.b_mid(self.down3(out3) + self.te_mid(t).reshape(n, -1, 1, 1))  # (N, 40, 3, 3)\n",
    "\n",
    "        out4 = torch.cat((out3, self.up1(out_mid)), dim=1)  # (N, 80, 7, 7)\n",
    "        out4 = self.b4(out4 + self.te4(t).reshape(n, -1, 1, 1))  # (N, 20, 7, 7)\n",
    "\n",
    "        out5 = torch.cat((out2, self.up2(out4)), dim=1)  # (N, 40, 14, 14)\n",
    "        out5 = self.b5(out5 + self.te5(t).reshape(n, -1, 1, 1))  # (N, 10, 14, 14)\n",
    "\n",
    "        out = torch.cat((out1, self.up3(out5)), dim=1)  # (N, 20, 28, 28)\n",
    "        out = self.b_out(out + self.te_out(t).reshape(n, -1, 1, 1))  # (N, 1, 28, 28)\n",
    "\n",
    "        out = self.conv_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def mlp_time_embedding(dim_in: int, dim_out:int)-> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_in, dim_out),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(dim_out, dim_out)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many trainable parameters do we have ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion module\n",
    "\n",
    "We choosed to use a nn.Module object for the diffusion process. Here is the main important challenge. \n",
    "In a diffusion process, we have to take care on:\n",
    "- beta variance schedule (we will go to the simple case where $\\beta_t$ is linear in $t$ like in the original paper but you cantry others schedules (for instance this [a cosine scheduler](https://arxiv.org/abs/2102.09672))) \n",
    "- code $\\alpha_t := 1-\\beta_t$ and $\\bar{\\alpha}_t := \\prod_{i=1}^t \\alpha_i$\n",
    "\n",
    "<div>\n",
    "<img src=\"images/parametrization_trick.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "- The make_noisy function that given images at time 0 (no noise) add noise until the step $t$.\n",
    "- The forward function (to train the model)\n",
    "- The sampling function, that will be used to generate images\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"images/training_sampling.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion(nn.Module):\n",
    "    def __init__(self, model: nn.Module, n_times: int=1000, beta_minmax:tuple[float, float]=[1e-4, 2e-2], device: str='mps'):\n",
    "    \n",
    "        super(Diffusion, self).__init__()\n",
    "    \n",
    "        self.n_times = n_times\n",
    "\n",
    "        self.model = model\n",
    "        \n",
    "        # define betas variance schedule\n",
    "\n",
    "        # define alpha for forward diffusion kernel\n",
    "        \n",
    "        self.device = device\n",
    "    \n",
    "    def extract(self, a: torch.Tensor, t:torch.Tensor, x_shape:tuple[int, ...])-> torch.Tensor:\n",
    "        \"\"\"\n",
    "            This function will be used to extract alphas and betas at time t.\n",
    "            Basically, it gets the value of t, select the corresponding value of the tensor a\n",
    "            at the index t, and reshape it to match the value of x.\n",
    "            This tensor's broadcasting helps for hadamard product.\n",
    "            from lucidrains' implementation\n",
    "                https://github.com/lucidrains/denoising-diffusion-pytorch/blob/beb2f2d8dd9b4f2bd5be4719f37082fe061ee450/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py#L376        \"\"\"\n",
    "        b, *_ = t.shape\n",
    "        out = a.gather(-1, t)\n",
    "        return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "    \n",
    "    def scale_to_minus_one_to_one(self, x: torch.Tensor)->torch.Tensor:\n",
    "        # according to the DDPMs paper, normalization seems to be crucial to train reverse process network\n",
    "        return x * 2 - 1\n",
    "    \n",
    "    def reverse_scale_to_zero_to_one(self, x:torch.Tensor)->torch.Tensor:\n",
    "        # reverse the normalization\n",
    "        return (x + 1) * 0.5\n",
    "    \n",
    "    def make_noisy(self, x_zeros: torch.Tensor, t: torch.Tensor)->tuple[torch.Tensor, torch.Tensor]:\n",
    "        # In this function, you should, given $x_{t=0}$ generate $x_t$, i.e. generate a noisy\n",
    "        # sample. This function will be used to create inputs during the training so no need to pass\n",
    "        # any gradients here.\n",
    "        # Please return $x_t$ and $\\epsilon$.\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, x_zeros:torch.Tensor)-> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Forward function. Given a not noisy sample $x_{t=0}$, generate a noisy sample $x_t$.\n",
    "        # t should be sampled uniformly from $[1, T]$.\n",
    "        # \\epsilon is given by the function make_noisy.\n",
    "        # return the noisy samples $x_t$, epsilon and the predicted noise $\\epsilon$.\n",
    "        pass\n",
    "    \n",
    "    def denoise_at_t(self, x_t:torch.Tensor, timestep:torch.Tensor, t:int)-> torch.Tensor:\n",
    "        # Denoise function. should be used to denoise a sample $x_t$ to time $t-1$.\n",
    "        pass\n",
    "\n",
    "    def sample(self, batch_size:int)-> torch.Tensor:\n",
    "        # start from random noise vector, x_0\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyUNet(time_emb_dim=256)\n",
    "\n",
    "diffusion = Diffusion(model, n_times=1000, device=DEVICE).to(DEVICE)\n",
    "\n",
    "optimizer = Adam(diffusion.parameters(), lr=3e-4)\n",
    "denoising_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch_idx, (x, _) in enumerate(val_dataloader):\n",
    "    x = x.to(DEVICE)\n",
    "    perturbed_images, epsilon, pred_epsilon = diffusion(x)\n",
    "    perturbed_images = diffusion.reverse_scale_to_zero_to_one(perturbed_images)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(perturbed_images.cpu()[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training DDPMs...\")\n",
    "model.train()\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    noise_prediction_loss = 0\n",
    "    for batch_idx, (x, _) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = x.to(DEVICE)\n",
    "        \n",
    "        noisy_input, epsilon, pred_epsilon = diffusion(x)\n",
    "        loss = denoising_loss(pred_epsilon, epsilon)\n",
    "        \n",
    "        noise_prediction_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tDenoising Loss: \", noise_prediction_loss / batch_idx)\n",
    "    \n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_images = diffusion.sample(N=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(generated_images[9][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpm_mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
